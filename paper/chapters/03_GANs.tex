\chapter{Generative Adversarial Networks Introduction}\label{chapter:gans}
Generative Adversarial Network (GAN)\cite{goodfellow2014generative} is a
machine learning framework designed to estimate generative models using
adversarial process. At the core it consists of two models: the generative one
$G$ which is capable of learning the distribution of the input data and
discriminative model $D$ that, given a data point, estimates whether it comes
from the input data or was generated by $G$. The models are set to compete with each
other in a minmax game. $D$ is trained to maximize the probability of correctly
distinguishing between the generated and input samples, while $G$ is trained to
minimize it.  

To approximate the true distribution $p_r$ over data $X$ we define a prior noise
distribution $p_z$ over noise input $Z$ and the generator distribution $p_g$
over data $X$. The generator represents a mapping
$G(z \sim Z; \theta_g) \to x \sim X$, where $\theta_g$ is some learnable
parameter (summary in Table \ref{tab:gan_probabilities}).

The discriminator $D(x, \theta_d) \to [0;1]$ is a function that given a sample $x
\sim X$ outputs the probability whether $x$ comes from data or was generated by
$G$. 

In classical GANs both $G$ and $D$ are most often modeled as multi-layer
perceptrons\cite{goodfellow2014generative} or other types of neural networks
(e.g. convolutional neural networks \cite{radford2016unsupervised}). However,
there exist many different variations of cost functions used during the mixmax
training procedure. In the following paragraphs we take a closer look at two of
them, namely SGANs and WGANs, which are the most relevant in the context of this
work. To simplify the notation, we skip the $\theta$ parameters, i.e. $G(z,
\theta_g) = G(z)$ and $D(x, \theta_d) = D(x)$
\begin{table}[]
  \centering
  \begin{tabular}{|l|l|}
    \hline
    Probability Distribution & Description  \\ \hline
    $p_z$ & True distribution over noise input $Z$ \\ \hline
    $p_g$&  Approximated distribution over input data $X$ given by $G$ \\ \hline
    $p_r$&  True distribution over input data $X$\\ \hline
  \end{tabular}
  \caption{\label{tab:gan_probabilities} Description of the probability
    distributions used in GANs}
\end{table}

\section{Standard Generative Adversarial Networks (SGANs)}
The goal of $D$ is to distinguish between the real and generated samples. For $x
\sim X$ the output $D(x)$ should approach $1$, while for $x \sim G(z)$ it should
approach $0$. In other terms it simultaneously maximizes $\mathbb{E}_{x \sim
  p_r(x)}[\log{D(x)}]$, and $\mathbb{E}_{z \sim p_z(z)}[\log{(1 - D(G(z)))}]$.

Generator $G$ has an opposite objective, thus it minimizes the $\mathbb{E}_{z
  \sim p_z(z)}[\log{1 - D(G(z))}]$.

Putting it all together, we get the objective for SGANs \ref{eq:SGANs_objective}.
\begin{equation}
  \label{eq:SGANs_objective}
  \begin{split}
    \min_G\max_D\mathcal{L(G, D)} & = \min_G\max_D \mathbb{E}_{x \sim p_r(x)}[\log{D(x)}] +  \mathbb{E}_{z \sim p_z(z)}[\log{1 - D(G(z))}] \\
    & = \min_G\max_D \mathbb{E}_{x \sim p_r(x)}[\log{D(x)}] +  \mathbb{E}_{x \sim p_g(x)}[\log{1 - D(x)}]
  \end{split}
\end{equation}
It can be shown, that if $D$ is optimal, this loss function measures the similarity between 
$p_r$ and $p_g$ according to Jensen–Shannon divergence (see Appendix
\ref{apx:JSD} for detailed analysis). 

SGANs are a very powerful tool, however, the training process is very difficult
and unstable \cite{salimans2016improved}. Additionally, if the data exists in
low dimensional manifold (which seems to be the case for most real world data
\cite{narayanan2010proceedings}), $p_r$ and $p_g$ are very likely disjoint and we are always capable of finding the perfect discriminator 
\cite{arjovsky2017principled}. This might seem like a good
characteristic, but if we examine the loss function closer, we find that it
makes generator incapable of learning. For the perfect $D$ we have $\forall x
\sim X, D(x) = 1$ and $\forall z \sim Z, D(G(z)) = 0$. For those values
$\mathcal{L(G,D)} = 0$ gradient vanishes and the gradient based optimizers cannot improve the
generator anymore.
\section{Wasserstein Generative Adversarial Networks (WGANs)}
One of the very prominent improvement to SGANs and a way to mitigate the
vanishing gradient problem is changing the cost function to use Wasserstein
Distance.

The Wasserstein Distance, also called Earth-Mover's Distance is a distance
measure between two probability distributions. The name ``Earth Mover'' comes
from the fact, that informally the distance can be described as follows: Given
two probability distributions, if we imagine them as piles of dirt, the ``Earth
Mover'' distance says what is the minimum cost of turning one pile of dirt into the
other one. Where cost is the volume that has to be moved times the distance is
has to be moved.

The main advantage of the Wasserstein Distance over Jensen–Shannon Divergence is,
that even in the case of disjoint distribution we get meaningful, stable
distance which is well suited for gradient based learning.

Formally, the Wasserstein or Earth-Mover's Distance (EM Distance) is defined as
follows:

\begin{equation}
  \label{eq:EMD}
  W(p_r, p_g) = \inf_{\gamma \in \Pi(p_r, p_g)} \mathbb{E}_{(x, y) \sim \gamma}[\norm{x-y}]
\end{equation}

Where $\Pi(p_r, p_g)$ denotes the set of all possible joint distributions,
$\gamma(x,y)$ says how much ``dirt'' has to be transported in order to transform
$p_g$ into $p_r$. Since we take $\inf$, the EM Distance is then the cost of such
transport done in the optimal way. 

The computation in \ref{eq:EMD} is highly intractable, but according to
Kantorovich-Rubinstein duality \cite{villani@optimal}, we can rewrite it as
follows.

\begin{equation}
  \label{eq:EMD2}
  W(p_r, p_g) = \frac{1}{K} \sup_{\norm{f}_L < K} \mathbb{E}_{x \sim p_r}[f(x)] - \mathbb{E}_{x \sim p_g}[f(x)]
\end{equation}

Where the supremum is overall all K-Lipschitz functions $f$. In practice it is
impossible to go over all such functions. Instead we define a family of parameterized
functions $\{f_{w}\}_{w \in W}$ and using the same notation for the generator as
in SGANs the WGANs objective becomes


\begin{equation}
  \label{eq:EMDL}
  \max_{f}\min_{G}\mathcal{L}(f, G) = \max_{f}\min_{G}  \mathbb{E}_{x \sim p_r}[f_w(x)] - \mathbb{E}_{z \sim p_z}[f_w(G_{\theta}(z))].
\end{equation}

Here the function $f_w$ represents the ``discriminator'', however it does not
anymore tries to distinguish between real and fake samples. Instead it is
trained to to approximate K-continous Lipschitz function and compute the
Wasserstein distance. The one missing part from the Equation \ref{eq:EMDL} is
ensuring the $f_w$ Lipschitz continuity, this can be achieved by gradient
clipping \cite{arjovsky2017wasserstein} or gradient penalty \cite{gulrajani2017improved}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End: